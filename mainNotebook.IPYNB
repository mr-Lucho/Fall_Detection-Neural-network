{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow.lite\n",
    "# tf.lite.TFLiteConverter.from_keras_model\n",
    "print(tensorflow.__version__)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "tensorflow.random.set_seed(2137)\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "# from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def print_signals(signal:np.ndarray):\n",
    "    '''\n",
    "    @Deprecated, util to nicely plot signal of width 9\n",
    "    :param signal: one complete sequence of any length with width 9'''\n",
    "    plt.figure(num=1,figsize=(30,40))\n",
    "    colours = ['r','g','b']\n",
    "    plt.subplot(3,1,1)\n",
    "    for i,color in enumerate(colours):\n",
    "        plt.plot(signal[:,i],color)\n",
    "    plt.subplot(3,1,2)\n",
    "    for i,color in enumerate(colours):\n",
    "        plt.plot(signal[:,i+3],color)\n",
    "    plt.subplot(3,1,3)\n",
    "    for i,color in enumerate(colours):\n",
    "        plt.plot(signal[:,i+6],color)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def printSignals(signal):\n",
    "    '''\n",
    "    util to nicely plot signal of width 8 with proper legend and labels\n",
    "    :param signal: one complete sequence of any length with width 8'''\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10, 5))\n",
    "    \n",
    "    ax[0].plot(signal[:,0],label='Acc x')\n",
    "    ax[0].plot(signal[:,1],label='Acc y')\n",
    "    ax[0].plot(signal[:,2],label='Acc z')\n",
    "\n",
    "    ax[1].plot(signal[:,3],label='Rot x')\n",
    "    ax[1].plot(signal[:,4],label='Rot y')\n",
    "    ax[1].plot(signal[:,5],label='Rot z')\n",
    "\n",
    "    ax[2].plot(signal[:,7],label='Roll')\n",
    "    ax[2].plot(signal[:,8],label='Yaw')\n",
    "\n",
    "    ax[0].legend(loc='upper right')\n",
    "    ax[1].legend(loc='upper right')\n",
    "    ax[2].legend(loc='upper right')\n",
    "\n",
    "    ax[0].set_ylabel('Wielokrotności g')\n",
    "    ax[1].set_ylabel('stopnie/s')\n",
    "    ax[2].set_ylabel('stopnie')\n",
    "\n",
    "    ax[0].grid(True)\n",
    "    ax[1].grid(True)\n",
    "    ax[2].grid(True)\n",
    "\n",
    "    fig.suptitle('Wybrany przebieg sygnału odpowiadający codziennej czynności')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def getDataPredictions(data,someModel):\n",
    "    y_predicted = []\n",
    "    for elem in data:\n",
    "        elem = elem.reshape([1,window_size_in_samples,8])\n",
    "        y_predicted.append(int(someModel.predict(elem,verbose=0)[0][0] > 0.5))\n",
    "    return y_predicted\n",
    "\n",
    "def plotConf(mat):\n",
    "    sns.set(font_scale=1.2)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(mat, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"size\": 40}, cbar=False)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Predicted Labels\", fontsize=16)\n",
    "    plt.ylabel(\"True Labels\", fontsize=16)\n",
    "    plt.title(\"Confusion Matrix\", fontsize=18)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions of global variables\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "lst_to_compute_scaling =[]\n",
    "window_size_in_samples = 70\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(num:int) -> tuple:\n",
    "    '''\n",
    "    Transform subject file with a given number into tuple. Performs removal of unnecessary colums and axis swap.\n",
    "    :param num: index of subject file, must be between 1 and 17\n",
    "    :returns x_falls: list of all separate sequences\n",
    "    :returns y_falls: corresponding list indicating a sequence is a fall \n",
    "    '''\n",
    "    path = f'fall-dataset/fall-dataset-raw/Subject{num}-raw.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df.drop(columns='Timestamp',inplace=True)\n",
    "    df.drop(columns='Pitch',inplace=True)\n",
    "    \n",
    "    x_falls = []\n",
    "    y_falls = []\n",
    "    \n",
    "    swapAxisInDf(df)\n",
    "    lst_to_compute_scaling.extend(df.values[:,1:9])\n",
    "    separated_features_dict = dict(tuple(df.groupby('Feature Line')))\n",
    "    for _, sub_df in separated_features_dict.items():\n",
    "        sub_df.drop(columns='Feature Line',inplace=True)\n",
    "        y_falls.append(np.uint8(sub_df.iloc[0,-1]))\n",
    "        sub_df.drop(columns='Fall',inplace=True)\n",
    "        x_falls.append(sub_df.values)\n",
    "    return x_falls, y_falls\n",
    "\n",
    "def swapAxisInDf(df:pd.DataFrame):\n",
    "    '''swaps axis orientation for training neural network taking into account phone axis'''\n",
    "    df['Acc(X)'] = df['Acc(Y)'] * -1\n",
    "    df['Acc(Y)'] = df['Acc(Z)']\n",
    "    df['Acc(Z)'] = df['Acc(X)']\n",
    "\n",
    "    df['Rot(X)'] = df['Rot(Y)'] * -1\n",
    "    df['Rot(Y)'] = df['Rot(Z)']\n",
    "    df['Rot(Z)'] = df['Rot(X)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_data(nums_for_validation:list,nums_for_testing):\n",
    "    '''\n",
    "    goes through every subject and extends lists for training validating and testing,\n",
    "    unspecified numbers will be used for training,\n",
    "    lists should not contain the same number (sets should be independent of each other)\n",
    "    param: nums_for_validation: list of numbers of subjects whose data will be used for validating algorithm\n",
    "    param: nums_for_validation: list of numbers of subjects whose data will be used for testing algorithm  '''\n",
    "\n",
    "    if any([x_test,y_test,x_val,y_val,x_train,y_train]):\n",
    "        print('data already initialized')\n",
    "        return\n",
    "    for i in range(1,18):\n",
    "        x_part, y_part = rewrite(i)\n",
    "\n",
    "        if i in nums_for_testing:\n",
    "            x_test.extend(x_part)\n",
    "            y_test.extend(y_part)\n",
    "        elif i in nums_for_validation:\n",
    "            x_val.extend(x_part)\n",
    "            y_val.extend(y_part)\n",
    "        else:\n",
    "            x_train.extend(x_part)\n",
    "            y_train.extend(y_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_all_data([4,17,9,15],[2,6,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_of_col = [] \n",
    "'''variable holding min max column values as list of tuples (min, max) '''\n",
    "lst_to_compute_scaling = np.array(lst_to_compute_scaling)\n",
    "for column_idx in range(0,8):\n",
    "    min_max_of_col.append((lst_to_compute_scaling[:, column_idx].min(),lst_to_compute_scaling[:, column_idx].max()))\n",
    "print(min_max_of_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_column(col,min_,max_):\n",
    "    '''applies desired scaling to every element in np array column\n",
    "    :parameter col: column array\n",
    "    :returns transformed array\n",
    "    '''\n",
    "    temp = []\n",
    "    for item in col:\n",
    "        res = 2 * (item - min_)/ (max_ - min_) -1\n",
    "        temp.append(res)\n",
    "    return temp\n",
    "\n",
    "def scale(x_lst,min_max_of_col):\n",
    "    '''applies scaling defined in transform_column() to every column in sequence and every sequence, works with differing sequence lengths but requires matching params\n",
    "    param: x_list: list of sequences\n",
    "    param: min_max_of_col: list containing tuple of (min,max) in order for every column in sequences of x_list'''\n",
    "    for record in x_lst:\n",
    "        num_rows, num_cols = record.shape\n",
    "        for col_idx in range(num_cols):\n",
    "            record[:,col_idx] = transform_column(record[:,col_idx],(min_max_of_col[col_idx])[0],(min_max_of_col[col_idx])[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale(x_train,min_max_of_col)\n",
    "scale(x_val,min_max_of_col) \n",
    "scale(x_test,min_max_of_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCustomPad(x_arr,window_size_in_samples):\n",
    "    max_rows = max(arr.shape[0] for arr in x_arr)\n",
    "    max_cols = max(arr.shape[1] for arr in x_arr)\n",
    "    padded_array = []\n",
    "    for sequence in x_arr:\n",
    "        rows_to_pad = max_rows - sequence.shape[0]\n",
    "        cols_to_pad = max_cols - sequence.shape[1]\n",
    "        padded_sequence = np.pad(sequence, ((0, rows_to_pad), (0, cols_to_pad)), mode='constant', constant_values=0)\n",
    "        padded_array.append(padded_sequence)\n",
    "    ret = np.stack(padded_array,axis=0)\n",
    "    ret = ret[:,:window_size_in_samples,:] \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = myCustomPad(x_train,window_size_in_samples)\n",
    "x_test = myCustomPad(x_test,window_size_in_samples)\n",
    "x_val = myCustomPad(x_val,window_size_in_samples)\n",
    "\n",
    "y_val = np.array(y_val,dtype=np.float32)\n",
    "y_train = np.array(y_train,dtype=np.float32)\n",
    "y_test = np.array(y_test,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel(tensorflow.keras.callbacks.Callback):\n",
    "    def __init__(self, save_best_metric='val_loss', this_max=False):\n",
    "        self.save_best_metric = save_best_metric\n",
    "        self.max = this_max\n",
    "        if this_max:\n",
    "            self.best = float('-inf')\n",
    "        else:\n",
    "            self.best = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metric_value = logs[self.save_best_metric]\n",
    "        if self.max:\n",
    "            if metric_value > self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_weights = self.model.get_weights()\n",
    "                print(f'got best model in: {epoch} epoch')\n",
    "\n",
    "        else:\n",
    "            if metric_value < self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_weights= self.model.get_weights()\n",
    "                print(f'got best model in: {epoch} epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here specify wchich model would you like to build \n",
    "build = 1\n",
    "input_shape = (window_size_in_samples,8)\n",
    "plotModel = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "if build ==1:\n",
    "    model.add(LSTM(units=40, return_sequences=False, input_shape=input_shape,stateful=False))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    save_best_model = SaveBestModel()\n",
    "    # history = model.fit(x_train,y_train,epochs=50,validation_data=(x_val, y_val),batch_size=45,verbose=0,callbacks=[save_best_model])\n",
    "    # model.set_weights(save_best_model.best_weights)\n",
    "    model.load_weights('Weights_folder1/Weights')\n",
    "    \n",
    "elif build ==2:\n",
    "    model.add(LSTM(units=20, return_sequences=True, input_shape=input_shape,stateful=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(units=20, return_sequences=False, input_shape=input_shape,stateful=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=0.0002)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    save_best_model = SaveBestModel()\n",
    "    # history = model.fit(x_train,y_train,epochs=60,validation_data=(x_val, y_val),batch_size=45,verbose=0,callbacks=[save_best_model])\n",
    "    # model.set_weights(save_best_model.best_weights)\n",
    "    model.load_weights('Weights_folder2/Weights')\n",
    "\n",
    "elif build ==3:\n",
    "    model.add(LSTM(units=20, return_sequences=True, input_shape=input_shape,stateful=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=20, return_sequences=True, input_shape=input_shape,stateful=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=20, return_sequences=False, input_shape=input_shape,stateful=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    save_best_model = SaveBestModel()\n",
    "    # history = model.fit(x_train,y_train,epochs=50,validation_data=(x_val, y_val),batch_size=45,verbose=0,callbacks=[save_best_model])\n",
    "    # model.set_weights(save_best_model.best_weights)\n",
    "    model.load_weights('Weights_folder3/Weights')\n",
    "\n",
    "if plotModel ==1:\n",
    "    tensorflow.keras.utils.plot_model(\n",
    "        model,\n",
    "        to_file=f'modelPics/model{build}.png',\n",
    "        show_shapes=True,\n",
    "        show_dtype=False,\n",
    "        show_layer_names=False,\n",
    "        rankdir='TB',\n",
    "        expand_nested=False,\n",
    "        dpi=96,\n",
    "        layer_range=None,\n",
    "        show_layer_activations=True,\n",
    "        show_trainable=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train,y_train,verbose=1)\n",
    "model.evaluate(x_val,y_val,verbose=1)\n",
    "model.evaluate(x_test,y_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history in not None:\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['trening', 'walidacja'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['trening', 'walidacja'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = getDataPredictions(x_test,model)\n",
    "cm1 = confusion_matrix(y_test,y_predicted)\n",
    "plotConf(cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "print ('Accuracy : ', accuracy)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "prec = precision_score(y_test,y_predicted)\n",
    "print('precision : ',prec)\n",
    "\n",
    "f1 = f1_score(y_test,y_predicted)\n",
    "print('f1 : ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel = 0\n",
    "if saveModel ==1:\n",
    "  # path=f'Weights_folder{build}/Weights'\n",
    "  # model.save_weights(path)\n",
    "\n",
    "  converter = tensorflow.lite.TFLiteConverter.from_keras_model(model)\n",
    "  converter.target_spec.supported_ops = [\n",
    "    tensorflow.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "    tensorflow.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "  ]\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "  # Save the TFLite model to a file\n",
    "  with open(f'tflites/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "  # with open(f'tflites/best{build}Layer.tflite', 'wb') as f:\n",
    "  #     f.write(tflite_model)\n",
    "else:\n",
    "  print('not set to save anything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''results here are highly volatile and sometimes unrelayable,\n",
    " use only to get generic info about model behaviour, \n",
    "best params were picked here and later manually adjusted and tested in code above,\n",
    " watch out one LSTM row executes around 20 min on one layer model, \n",
    " on (-1 1) range of values model behaves much better '''\n",
    "class SaveBestModel(tensorflow.keras.callbacks.Callback):\n",
    "    def __init__(self, save_best_metric='val_loss', this_max=False):\n",
    "        self.save_best_metric = save_best_metric\n",
    "        self.max = this_max\n",
    "        if this_max:\n",
    "            self.best = float('-inf')\n",
    "        else:\n",
    "            self.best = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metric_value = logs[self.save_best_metric]\n",
    "        if self.max:\n",
    "            if metric_value > self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_weights = self.model.get_weights()\n",
    "                # print(f'got best model in: {epoch} epoch')\n",
    "\n",
    "        else:\n",
    "            if metric_value < self.best:\n",
    "                self.best = metric_value\n",
    "                self.best_weights= self.model.get_weights()\n",
    "                # print(f'got best model in: {epoch} epoch')\n",
    "\n",
    "\n",
    "for lstm in [5, 10, 15, 20, 25, 30, 40, 50, 90]:\n",
    "    for dropout in [0.1, 0.2, 0.3, 0.4]:\n",
    "        for learning in [0.001, 0.0005, 0.00025]:\n",
    "            for batchSize in [15, 45, 90]:\n",
    "                model = Sequential()\n",
    "                model.add(LSTM(units=lstm, return_sequences=True, input_shape=input_shape,stateful=False))\n",
    "                model.add(Dropout(dropout))\n",
    "                model.add(LSTM(units=lstm, return_sequences=True,stateful=False))\n",
    "                model.add(Dropout(dropout))\n",
    "                model.add(LSTM(units=lstm, return_sequences=False,stateful=False))\n",
    "                model.add(Dropout(dropout))\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                optimizer = Adam(learning_rate=learning)\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "                save_best_model = SaveBestModel()\n",
    "                history = model.fit(x_train,y_train,epochs=50,validation_data=(x_val, y_val),batch_size=batchSize,verbose=0,callbacks=[save_best_model])\n",
    "                model.set_weights(save_best_model.best_weights)\n",
    "                y_predicted = getDataPredictions(x_test,model)\n",
    "                cm1 = confusion_matrix(y_test,y_predicted)\n",
    "                accuracy = accuracy_score(y_test, y_predicted)\n",
    "                print (f'lstm={lstm}, dropout={dropout},leaning={learning}, batch={batchSize} Accuracy : ', accuracy)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
